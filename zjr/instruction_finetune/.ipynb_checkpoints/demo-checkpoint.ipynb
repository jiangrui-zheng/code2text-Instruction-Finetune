{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/filesystem/miniconda3/envs/llama_factory/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading [config.json]: 100%|██████████| 654/654 [00:00<00:00, 766B/s]\n",
      "Downloading [configuration.json]: 100%|██████████| 48.0/48.0 [00:00<00:00, 54.4B/s]\n",
      "Downloading [generation_config.json]: 100%|██████████| 187/187 [00:00<00:00, 192B/s]\n",
      "Downloading [LICENSE]: 100%|██████████| 7.62k/7.62k [00:00<00:00, 8.97kB/s]\n",
      "Downloading [model-00001-of-00004.safetensors]: 100%|██████████| 4.63G/4.63G [00:23<00:00, 208MB/s] \n",
      "Downloading [model-00002-of-00004.safetensors]: 100%|██████████| 4.66G/4.66G [00:23<00:00, 212MB/s] \n",
      "Downloading [model-00003-of-00004.safetensors]: 100%|██████████| 4.58G/4.58G [00:28<00:00, 170MB/s] \n",
      "Downloading [model-00004-of-00004.safetensors]: 100%|██████████| 1.09G/1.09G [00:08<00:00, 137MB/s] \n",
      "Downloading [model.safetensors.index.json]: 100%|██████████| 23.4k/23.4k [00:00<00:00, 31.5kB/s]\n",
      "Downloading [README.md]: 100%|██████████| 36.3k/36.3k [00:00<00:00, 53.0kB/s]\n",
      "Downloading [special_tokens_map.json]: 100%|██████████| 73.0/73.0 [00:00<00:00, 107B/s]\n",
      "Downloading [tokenizer.json]: 100%|██████████| 8.66M/8.66M [00:00<00:00, 10.0MB/s]\n",
      "Downloading [tokenizer_config.json]: 100%|██████████| 49.8k/49.8k [00:00<00:00, 65.2kB/s]\n",
      "Downloading [USE_POLICY.md]: 100%|██████████| 4.59k/4.59k [00:00<00:00, 5.50kB/s]\n"
     ]
    }
   ],
   "source": [
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct')\n",
    "# print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/modelscope/hub/LLM-Research/Meta-Llama-3-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 切换为你下载的模型文件目录, 这里的demo是Llama-3-8B-Instruct\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 如果是其他模型，比如qwen，chatglm，请使用其对应的官方demo\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# 切换为你下载的模型文件目录, 这里的demo是Llama-3-8B-Instruct\n",
    "# 如果是其他模型，比如qwen，chatglm，请使用其对应的官方demo\n",
    "model_id = \"/workspace/instruct_finetune_llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "diff = \"\"\"\n",
    "diff --git a/core/filter_form_api.php b/core/filter_form_api.php\n",
    "index 05d5c39e1e..7b38752fdf 100644\n",
    "--- a/core/filter_form_api.php\n",
    "+++ b/core/filter_form_api.php\n",
    "@@ -2393,10 +2393,9 @@ function filter_form_draw_inputs( $p_filter, $p_for_screen = true, $p_static = f\n",
    " \t}\n",
    " \n",
    " \tif( null === $p_static_fallback_page ) {\n",
    "-\t\t$p_static_fallback_page = $_SERVER['PHP_SELF'];\n",
    "-\t\t$p_static_fallback_page = string_sanitize_url( $_SERVER['PHP_SELF'] );\n",
    "+\t\t$p_static_fallback_page = $_SERVER['SCRIPT_NAME'];\n",
    " \t}\n",
    "-\t$t_filters_url = $p_static_fallback_page;\n",
    "+\t$t_filters_url = helper_mantis_url( $p_static_fallback_page );\n",
    " \t$t_get_params = $_GET;\n",
    " \t$t_get_params['for_screen'] = $p_for_screen;\n",
    " \t$t_get_params['static'] = ON;\n",
    "\"\"\"\n",
    "\n",
    "gt_tokens = \"view  _  fil  t  _  page .  phpmanage  _  filter  _  edit  _  page .  php  Fix  es\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant that explains code differences (diffs) in detail. Your task is to map each variable or function from the code to its corresponding line number and explain the changes made in the code. The explanations should focus on the purpose of the change and its impact on the code's behavior.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "     Here is a code difference (diff) along with a set of relevant tokens from the CVE description. Please provide a detailed explanation for each variable or function mentioned in the diff, including the line number, the changes made, and their significance. \n",
    "     Diff: {diff}\n",
    "     Relevant CVE tokens: {gt_tokens}\n",
    "     \"\"\"},]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    ")\n",
    "print(\"Generated Prompt:\\n\", prompt)\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print()\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
